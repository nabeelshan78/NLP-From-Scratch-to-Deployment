{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install torch==2.2.2 torchtext==0.17.2 nltk\n",
    "# !pip install torchdata==0.7.1\n",
    "# !pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "from model import ClassificationNet, train_epoch, evaluate_epoch, save_list_to_file, load_list_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported successfully!\n",
      "Running on device: cuda\n",
      "Loading data from Parquet files...\n",
      "Vocabulary Size: 95811\n",
      "\n",
      "Verifying by fetching one batch from train_dataloader...\n",
      "Labels batch shape: torch.Size([64])\n",
      "Texts batch shape: torch.Size([64, 74])\n"
     ]
    }
   ],
   "source": [
    "from dataloader import get_dataloaders, DEVICE\n",
    "\n",
    "print(\"Imported successfully!\")\n",
    "print(f\"Running on device: {DEVICE}\")\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_dataloader, valid_dataloader, test_dataloader, vocab = get_dataloaders(batch_size=BATCH_SIZE)\n",
    "\n",
    "# --- Verification Step ---\n",
    "print(\"\\nVerifying by fetching one batch from train_dataloader...\")\n",
    "labels, texts = next(iter(train_dataloader))\n",
    "\n",
    "print(f\"Labels batch shape: {labels.shape}\")\n",
    "print(f\"Texts batch shape: {texts.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "num_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment artifacts will be saved in: runs/adam_from_epoch51\n",
      "Metrics will be saved in: runs/adam_from_epoch51/metrics\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_DIR = \"runs/adam_from_epoch51\"\n",
    "METRICS_DIR = os.path.join(EXPERIMENT_DIR, \"metrics\") \n",
    "os.makedirs(EXPERIMENT_DIR, exist_ok=True)\n",
    "os.makedirs(METRICS_DIR, exist_ok=True) \n",
    "\n",
    "NEW_CHECKPOINT_PATH = os.path.join(EXPERIMENT_DIR, 'checkpoint.pth')\n",
    "\n",
    "print(f\"Experiment artifacts will be saved in: {EXPERIMENT_DIR}\")\n",
    "print(f\"Metrics will be saved in: {METRICS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model and optimizer\n",
    "model = ClassificationNet(vocab_size=vocab_size, num_class=num_classes).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Decay the learning rate by a factor of 10 every 8 epochs.\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "best_val_accuracy = 0.0\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CHECKPOINT_PATH = 'runs/initial/checkpoint_epoch_50.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found base checkpoint. Initializing model with weights from 'runs/initial/checkpoint_epoch_50.pth'...\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(BASE_CHECKPOINT_PATH):\n",
    "    print(f\"Found base checkpoint. Initializing model with weights from '{BASE_CHECKPOINT_PATH}'...\")\n",
    "    checkpoint = torch.load(BASE_CHECKPOINT_PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict']) # Load weights only\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    history = checkpoint['history']                       # Continue history\n",
    "    best_val_accuracy = checkpoint['best_val_accuracy']\n",
    "else:\n",
    "    print(\"No checkpoints found. Starting a completely new training run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to: runs/adam_from_epoch51/metrics/train_loss.pkl\n",
      "Data successfully saved to: runs/adam_from_epoch51/metrics/train_acc.pkl\n",
      "Data successfully saved to: runs/adam_from_epoch51/metrics/val_loss.pkl\n",
      "Data successfully saved to: runs/adam_from_epoch51/metrics/val_acc.pkl\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 70            # Train up to a total of 70 epochs\n",
    "\n",
    "print(f\"Starting training from epoch {start_epoch}...\")\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS + 1):\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    \n",
    "    # Run training and validation\n",
    "    train_loss, train_acc = train_epoch(model, train_dataloader, criterion, optimizer, DEVICE)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    \n",
    "    val_loss, val_acc = evaluate_epoch(model, valid_dataloader, criterion, DEVICE)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"\\nEpoch Summary:\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"\\tValid Loss: {val_loss:.4f} | Valid Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "    # Save best model to the experiment folder\n",
    "    if val_acc > best_val_accuracy:\n",
    "        best_val_accuracy = val_acc\n",
    "        best_model_path = os.path.join(EXPERIMENT_DIR, 'best_model.pth')\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"New best model saved to '{best_model_path}'\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Training Finished!\")\n",
    "        \n",
    "        \n",
    "# Save final history to the experiment folder\n",
    "for key, value in history.items():\n",
    "    file_path = os.path.join(METRICS_DIR, f\"{key}.pkl\")\n",
    "    save_list_to_file(value, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the checkpoint for this run \n",
    "current_checkpoint = {\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'history': history,\n",
    "    'best_val_accuracy': best_val_accuracy\n",
    "}\n",
    "torch.save(current_checkpoint, NEW_CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
